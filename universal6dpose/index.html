<html>

<head>
    <link rel="StyleSheet" href="style.css" type="text/css" media="all">
    <title>Universal Features Guided Zero-Shot Category-Level Object Pose Estimation</title>
    <meta property="og:title" content="Universal Features Guided Zero-Shot Category-Level Object Pose Estimation">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>

</head>

<body>
    <br>
    <div class="center-div">
        <span style="font-size:40px">Universal Features Guided Zero-Shot Category-Level Object Pose Estimation</span>
    </div>

    <br>
    <table align="center" width="1100px">
        <tbody>
            <tr>
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            Wentian Qu<sup>1,2,3</sup>
                        </span>
                    </div>
                </td>
                
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            Chenyu Meng<sup>1,2</sup>
                        </span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            Heng Li<sup>3</sup>
                        </span>
                    </div>
                </td>
                
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            Jian Cheng<sup>1,2</sup>
                        </span>
                    </div>
                </td>
                
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            Cuixia Ma<sup>1,2</sup>
                        </span>
                    </div>
                </td>

            </tr>
        </tbody>
    </table>


    <table align="center" width="800px">
        <tbody>
            
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            Hongan Wang<sup>1,2</sup>
                        </span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            Xiao Zhou<sup>4</sup>
                        </span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            <a href="http://www.idengxm.com/">Xiaoming Deng</a><sup>1,2</sup>*
                        </span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            <a href="https://ece.hkust.edu.hk/pingtan">Ping Tan</a><sup>3</sup>*
                        </span>
                    </div>
                </td>

                

            </tr>
        </tbody>
    </table>
    <br>

    <table align="center" width="1100px">
        <tbody>
            <tr>
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            <sup>1</sup>Institute of Software, Chinese Academy of Sciences</a>
                        </span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            <sup>2</sup>University of Chinese Academy of Sciences</a>
                        </span>
                    </div>
                </td>

            </tr>
        </tbody>
    </table>

    <table align="center" width="1100px">
        <tbody>
            <tr>
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            <sup>3</sup>Hong Kong University of Science and Technology</a>
                        </span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:22px">
                            <sup>4</sup>Aerospace Information Research Institute, Chinese Academy of Sciences</a>
                        </span>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    <br>
    <table align="center" width="600px">
        <tbody>
            <tr>
                <td align="center" width="150px">
                    <div class="center-div">
                        <img src="./resources/image/teaser.png" width="1100px"></a><br>
                    </div>
                </td>
            </tr>
            <tr>
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:14px">
                            (a) We propose a zero-shot pose estimation method for unseen categories using universal features and obtain accurate
                            results for multi-category scenes. Our method offers cost-efficient and superior generalization ability over traditional instance
                           level and category-level methods. (b) The correspondence with universal features degrades when pose has large gaps. (c) The
                            shape gap between objects will cause pose ambiguity in optimization. These challenges affect the accuracy of pose estimation.
                        </span>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>
    <br><br>

    <hr>
    <div class="center-div">
        <h1>Abstract</h1>
    </div>
    <p>
        Object pose estimation, crucial in computer vision and
        robotics applications, faces challenges with the diversity of
        unseen categories. We propose a zero-shot method to achieve
        category-level 6-DOF object pose estimation, which exploits
        both 2D and 3D universal features of input RGB-D image to 
        establish semantic similarity-based correspondences
        and can be extended to unseen categories without additional
        model fine-tuning. Our method begins with combining efficient 
        2D universal features to find sparse correspondences
        between intra-category objects and gets initial coarse pose.
        To handle the correspondence degradation of 2D universal
        features if the pose deviates much from the target pose, we
        use an iterative strategy to optimize the pose. Subsequently,
        to resolve pose ambiguities due to shape differences between
        intra-category objects, the coarse pose is refined by optimizing 
        with dense alignment constraint of 3D universal features.
        Our method outperforms previous methods on the REAL275
        and Wild6D benchmarks for unseen categories.
    </p>
    <br><br>

    <hr>
    <div class="center-div">
        <h1 id="code">Overview</h1>
    </div>

    <p>
        We exploits multi-modal (both 2D and 3D) universal
        features to estimate object pose on unseen categories.
        We design a coarse-to-fine framework for accurate 
        6-DOF pose estimation. At the coarse stage, it identifies 
        sparse correspondences to solve an initial coarse 
        object pose. Given an input RGB-D image, we use a reference 
        model of the interested category to render reference
        images and extract 2D universal features from both the target 
        and rendered reference images. We then calculate the cosine 
        similarity map between the 2D features and use cyclical
        distance to select Top-k correspondences. Combined with
        the depth map and camera intrinsics, we choose the Top-k
        keypoints in the camera coordinate and calculate the transformation 
        from the reference to the target space to get the
        initial coarse 6-DOF object pose by a least-squares solution.
        To deal with the problem of feature correspondence degradation 
        of 2D universal features if the initial pose deviates much
        from the target pose, we use an iterative strategy to optimize 
        the correspondence and coarse pose. After the coarse
         pose estimation, we map the reference model to the target
         image space to perform pose refinement with pixel-wise optimization. 
         In order to resolve pose ambiguities due to shape
         differences between intra-category objects during the optimization, 
         we employ 3D universal features extracted from
         the point cloud to refine the 6-DOF object pose and the reference 
         model iteratively by dense pixel-level registration.

    </p>

    <table align="center" width="1000px">
        <tbody>
            <tr>
                <td>
                    <div class="center-div">
                        <img class="round" style="width:1000" src="./resources/image/pipeline.png">
                    </div>
                </td>
            </tr>
            <tr>
                <td>
                    <div class="center-div">
                        <span style="font-size:14px">
                            Overview. Our framework includes a keypoint-level coarse pose estimation module and a pixel-level pose refinement
                            module. In the first module, we establish the correspondences between image pairs based on the 2D universal features and
                            calculate the coarse pose using least squares in an iterative manner. In the second module, we use pixel-level optimization
                            combined with 3D universal features to refine the pose and shape of reference model to obtain the fine pose.
                        </span>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    <table align="center" width="1000px">
        <tbody>
            <tr>
                <td>
                    <div class="center-div">
                        <img class="round" style="width:500" src="./resources/image/pose_refine.png">
                    </div>
                </td>
            </tr>
            <tr>
                <td>
                    <div class="center-div">
                        <span style="font-size:14px">
                            (a) Pose Refinement. Based on the coarse pose as
                            initialization, the reference model can be warped to the 
                            target space to obtain the initial mask and extract 3D 
                            universal features. Then we optimize the coarse pose and shape
                            by minimizing the loss function. (b) After pose refinement
                            stage, the pose and shape of the reference model are more
                            accurately aligned with the target object. 
                        </span>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    <br><br>

    <hr>
    <div class="center-div">
        <h1 id="paper">Paper and Code</h1>
    </div>
    <table align="center" width="900px">

        <tbody>
            <tr>
                <td>
                    <img class="layered-paper-big" style="height:175px" src="./resources/image/page1.png">
                </td>
                <td>
                    <span style="font-size:14pt">W. Qu, C. Meng, H. Li, J. Cheng, C. Ma, H. Wang, X. Zhou, X. Deng, P. Tan</span>
                    <br><br>
                    <b><span style="display:inline-block;width:600px;font-size:14pt">Universal Features Guided Zero-Shot Category-Level Object Pose Estimation</span></b>
                    <br><br>
                    <span style="font-size:14pt">AAAI, 2025</span>
                    <br><br>
                    <span style="font-size:20px">
                        <a href="https://arxiv.org/html/2501.02831v1">[Paper]</a> &nbsp; &nbsp;
                        <!-- <a href="./resources/bibtex.txt">[Bibtex]</a> &nbsp; &nbsp;  -->
                        <a href="https://iscas3dv.github.io/universal6dpose/">[Code, Coming Soon]</a> &nbsp; &nbsp;
                    </span>
                </td>
            </tr>
        </tbody>
    </table>
    <br><br>

    <hr>
    <div class="center-div">
        <h1 id="code">Results</h1>
    </div>

    <table align="center" width="1000px">
        <tbody>
            <tr>
                <td>
                    <div class="center-div">
                        <img class="round" style="width:1000" src="./resources/image/qual_res.png">
                    </div>
                </td>
            </tr>
            <tr>
                <td>
                    <div class="center-div">
                        <span style="font-size:14px">
                            Qualitative results on REAL275 and Wild6D. The red box represents the ground truth, 
                            and the green box represents the estimation. Previous methods exhibit large errors 
                            when applied to unseen categories due to the significant texture and shape differences. 
                            Our method demonstrates strong generalization on unseen categories with accurate pose estimation.
                        </span>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    
    
    <br><br>
    <hr>
    <table align="center" width="980px">
        <tbody>
            <tr>
                <td>
                    <left>
                        <div class="center-div">
                            <h1>Acknowledgements</h1>
                        </div>
                        <div class="center-div">
                            This work was supported in part by National Science and Technology Major Project (2022ZD0117904), 
                            National Nat-ural Science Foundation of China (62473356,62373061), Beijing Natural Science Foundation (L232028), 
                            and CAS Major Project (RCJJ-145-24-14). Heng Li and Ping Tan are supported by the project HKPC22EG01-E from the 
                            Hong Kong Industrial Artificial Intelligence & Robotics Centre (FLAIR).
                            The websiteis modified from this <a href="https://yqdch.github.io/DHSP3D/">template</a>.
                        </div>
                    </left>
                </td>
            </tr>
        </tbody>
    </table>
    <br>



</body>

</html>
